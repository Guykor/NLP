# -*- coding: utf-8 -*-
"""NLPex2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f0X-7TSXvakAF46OFYkGgr5ZSP5CZT0U
"""

# from google.colab import drive
# drive.mount('/content/drive')
#
# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Colab\ Notebooks

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import os
from torch.utils.data import DataLoader, TensorDataset, Dataset
import operator
import data_loader
import pickle
import matplotlib.pyplot as plt
import matplotlib as mpl
from tqdm.notebook  import tqdm

mpl.rcParams['figure.figsize'] = (10, 5)
colors = plt.rcParams['axes.prop_cycle'].by_key()['color']
# ------------------------------------------- Constants ----------------------------------------

SEQ_LEN = 52
W2V_EMBEDDING_DIM = 300

ONEHOT_AVERAGE = "onehot_average"
W2V_AVERAGE = "w2v_average"
W2V_SEQUENCE = "w2v_sequence"

TRAIN = "train"
VAL = "val"
TEST = "test"

LOSS = "loss"
ACCURACY = "accuracy"


# ------------------------------------------ Helper methods and classes --------------------------

def get_available_device():
    """
    Allows training on GPU if available. Can help with running things faster when a GPU with cuda is
    available but not a most...
    Given a device, one can use module.to(device)
    and criterion.to(device) so that all the computations will be done on the GPU.
    """
    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')


def save_pickle(obj, path):
    with open(path, "wb") as f:
        pickle.dump(obj, f)


def load_pickle(path):
    with open(path, "rb") as f:
        return pickle.load(f)


def save_model(model, path, epoch, optimizer):
    """
    Utility function for saving checkpoint of a model, so training or evaluation can be executed later on.
    :param model: torch module representing the model
    :param optimizer: torch optimizer used for training the module
    :param path: path to save the checkpoint into
    """
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict()}, path)


def load(model, path, optimizer):
    """
    Loads the state (weights, paramters...) of a model which was saved with save_model
    :param model: should be the same model as the one which was saved in the path
    :param path: path to the saved checkpoint
    :param optimizer: should be the same optimizer as the one which was saved in the path
    """
    checkpoint = torch.load(path)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    epoch = checkpoint['epoch']
    return model, optimizer, epoch


# ------------------------------------------ Data utilities ----------------------------------------

def load_word2vec():
    """ Load Word2Vec Vectors
        Return:
            wv_from_bin: All 3 million embeddings, each lengh 300
    """
    import gensim.downloader as api
    wv_from_bin = api.load("word2vec-google-news-300")
    vocab = list(wv_from_bin.vocab.keys())
    print(wv_from_bin.vocab[vocab[0]])
    print("Loaded vocab size %i" % len(vocab))
    return wv_from_bin


def create_or_load_slim_w2v(words_list, cache_w2v=False):
    """
    returns word2vec dict only for words which appear in the dataset.
    :param words_list: list of words to use for the w2v dict
    :param cache_w2v: whether to save locally the small w2v dictionary
    :return: dictionary which maps the known words to their vectors
    """
    w2v_path = "w2v_dict.pkl"
    if not os.path.exists(w2v_path):
        print("Creating Slim Word2Vec..")
        full_w2v = load_word2vec()
        w2v_emb_dict = {k: full_w2v[k] for k in words_list if k in full_w2v}
        if cache_w2v:
            save_pickle(w2v_emb_dict, w2v_path)
    else:
        print("Loading Slim Word2Vec...")
        w2v_emb_dict = load_pickle(w2v_path)
    print("DONE")
    return w2v_emb_dict


def get_w2v_average(sent, word_to_vec, embedding_dim):
    """
    This method gets a sentence and returns the average word embedding of the words consisting
    the sentence.
    :param sent: the sentence object
    :param word_to_vec: a dictionary mapping words to their vector embeddings
    :param embedding_dim: the dimension of the word embedding vectors
    :return The average embedding vector as numpy ndarray.
    """

    res = np.zeros(embedding_dim)
    sent_known_size = len(sent.text)
    for word in sent.text:
        if word in word_to_vec:
            res += word_to_vec[word]
        else:
            sent_known_size -= 1

    return torch.Tensor(res / np.max([1, sent_known_size]))


def get_one_hot(size, ind):
    """
    this method returns a one-hot vector of the given size, where the 1 is placed in the ind entry.
    :param size: the size of the vector
    :param ind: the entry index to turn to 1
    :return: numpy ndarray which represents the one-hot vector
    """
    res = np.zeros(size, dtype=float)
    res[ind] = 1.
    return res


def average_one_hots(sent, word_to_ind):
    """
    this method gets a sentence, and a mapping between words to indices, and returns the average
    one-hot embedding of the tokens in the sentence.
    :param sent: a sentence object.
    :param word_to_ind: a mapping between words to indices
    :return:
    """
    size = len(word_to_ind.keys())
    res = np.zeros(size)
    for word in sent.text:
        res += get_one_hot(size, word_to_ind[word])
    return torch.Tensor(res / len(sent.text))


def get_word_to_ind(words_list):
    """
    this function gets a list of words, and returns a mapping between
    words to their index.
    :param words_list: a list of words
    :return: the dictionary mapping words to the index
    """
    return {words_list[k]: k for k in np.arange(len(words_list))}


def sentence_to_embedding(sent, word_to_vec, seq_len, embedding_dim=300):
    """
    this method gets a sentence and a word to vector mapping, and returns a list containing the
    words embeddings of the tokens in the sentence.
    :param sent: a sentence object
    :param word_to_vec: a word to vector mapping.
    :param seq_len: the fixed length for which the sentence will be mapped to.
    :param embedding_dim: the dimension of the w2v embedding
    :return: numpy ndarray of shape (seq_len, embedding_dim) with the representation of the sentence
    """
    sent_words_embed = np.zeros((seq_len, embedding_dim))
    for ind, word in enumerate(sent.text):
        if ind >= seq_len:
            break
        if word in word_to_vec:
            sent_words_embed[ind] = word_to_vec[word]
    return sent_words_embed



class OnlineDataset(Dataset):
    """
    A pytorch dataset which generates model inputs on the fly from sentences of SentimentTreeBank
    """

    def __init__(self, sent_data, sent_func, sent_func_kwargs):
        """
        :param sent_data: list of sentences from SentimentTreeBank
        :param sent_func: Function which converts a sentence to an input datapoint
        :param sent_func_kwargs: fixed keyword arguments for the state_func
        """
        self.data = sent_data
        self.sent_func = sent_func
        self.sent_func_kwargs = sent_func_kwargs

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        sent = self.data[idx]
        sent_emb = self.sent_func(sent, **self.sent_func_kwargs)
        sent_label = sent.sentiment_class
        return sent_emb, sent_label


class DataManager():
    """
    Utility class for handling all data management task. Can be used to get iterators for training and
    evaluation.
    """

    def __init__(self, data_type=ONEHOT_AVERAGE, use_sub_phrases=True,
                 dataset_path="stanfordSentimentTreebank", batch_size=50,
                 embedding_dim=None):
        """
        builds the data manager used for training and evaluation.
        :param data_type: one of ONEHOT_AVERAGE, W2V_AVERAGE and W2V_SEQUENCE
        :param use_sub_phrases: if true, training data will include all sub-phrases plus the full sentences
        :param dataset_path: path to the dataset directory
        :param batch_size: number of examples per batch
        :param embedding_dim: relevant only for the W2V data types.
        """

        print("Loading data")
        # load the dataset
        self.sentiment_dataset = data_loader.SentimentTreeBank(dataset_path, split_words=True)
        # map data splits to sentences lists
        self.sentences = {}
        if use_sub_phrases:
            self.sentences[TRAIN] = self.sentiment_dataset.get_train_set_phrases()
        else:
            self.sentences[TRAIN] = self.sentiment_dataset.get_train_set()

        self.sentences[VAL] = self.sentiment_dataset.get_validation_set()
        self.sentences[TEST] = self.sentiment_dataset.get_test_set()

        # map data splits to sentence input preperation functions
        print("Prepare Embedding Functionality")
        words_list = list(self.sentiment_dataset.get_word_counts().keys())
        if data_type == ONEHOT_AVERAGE:
            self.sent_func = average_one_hots
            self.sent_func_kwargs = {"word_to_ind": get_word_to_ind(words_list)}
        elif data_type == W2V_SEQUENCE:
            self.sent_func = sentence_to_embedding

            self.sent_func_kwargs = {"seq_len": SEQ_LEN,
                                     "word_to_vec": create_or_load_slim_w2v(words_list),
                                     "embedding_dim": embedding_dim
                                     }
        elif data_type == W2V_AVERAGE:
            self.sent_func = get_w2v_average
            words_list = list(self.sentiment_dataset.get_word_counts().keys())
            self.sent_func_kwargs = {"word_to_vec": create_or_load_slim_w2v(words_list, True),
                                     "embedding_dim": embedding_dim
                                     }
        else:
            raise ValueError("invalid data_type: {}".format(data_type))
        # map data splits to torch datasets and iterators
        self.torch_datasets = {k: OnlineDataset(sentences, self.sent_func, self.sent_func_kwargs)
                               for
                               k, sentences in self.sentences.items()}
        self.torch_iterators = {k: DataLoader(dataset, batch_size=batch_size, shuffle=k == TRAIN)
                                for k, dataset in self.torch_datasets.items()}
        print("DONE")

    def get_torch_iterator(self, data_subset=TRAIN):
        """
        :param data_subset: one of TRAIN VAL and TEST
        :return: torch batches iterator for this part of the datset
        """
        return self.torch_iterators[data_subset]

    def get_labels(self, data_subset=TRAIN):
        """
        :param data_subset: one of TRAIN VAL and TEST
        :return: numpy array with the labels of the requested part of the datset in the same order of the
        examples.
        """
        return np.array([sent.sentiment_class for sent in self.sentences[data_subset]])

    def get_input_shape(self):
        """
        :return: the shape of a single example from this dataset (only of x, ignoring y the label).
        """
        return self.torch_datasets[TRAIN][0][0].shape


# ------------------------------------ Models ----------------------------------------------------

class LSTM(nn.Module):
    """
    An LSTM for sentiment analysis with architecture as described in the exercise description.
    """

    def __init__(self, embedding_dim, hidden_dim, n_layers, dropout):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.n_layers = n_layers
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, 
                            dropout=dropout if n_layers > 1 else 0, 
                            bidirectional=True, batch_first=True)
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_dim, 1)


    def forward(self, x):
        _, h_c_out = self.lstm(x.float())
        x = h_c_out[0].view(self.n_layers, 2, x.size(0), self.hidden_dim)
        x = torch.cat((x[:, 0, :, :], x[:, 1, :, :]), dim=0)[-1]
        if self.n_layers > 1:
            x = self.dropout(x)
        x = self.fc(x)
        return x

    def predict(self, x):
        with torch.no_grad():
            _, h_c_out = self.lstm(x.float())
            x = h_c_out[0].view(self.n_layers, 2, x.size(0), self.hidden_dim)
            x = torch.cat((x[:, 0, :, :], x[:, 1, :, :]), dim=0)[-1]
            x = self.fc(x)
            x = nn.Sigmoid()(x)
            return torch.round(x)


class LogLinear(nn.Module):
    """
    general class for the log-linear models for sentiment analysis.
    """

    def __init__(self, embedding_dim):
        super().__init__()
        self.linear = nn.Linear(embedding_dim, 1)
        self.activation = nn.Sigmoid()

    def forward(self, x):
        return self.linear(x)

    def predict(self, x):
        with torch.no_grad():  # temporarily sets x.requires_grad = False
            x = self.linear(x)
            x = self.activation(x)
            return torch.round(x)


# ------------------------- training functions --------------------- #

def binary_accuracy(preds, y):
    """
    This method returns the accuracy of the predictions, relative to the labels.
    You can choose whether to use numpy arrays or tensors here.
    :param preds: a vector of predictions
    :param y: a vector of true labels
    :return: scalar value - (<number of accurate predictions> / <number of examples>)
    """
    return (preds == y).float().mean()


def train_epoch(model, data_iterator, optimizer, criterion, epoch_num):
    """
    This method operates one epoch (pass over the whole train set) of training of the given model,
    and returns the accuracy and loss for this epoch.
    :param model: the model we're currently training
    :param data_iterator: an iterator, iterating over the training data for the model.
    :param optimizer: the optimizer object for the training process.
    :param criterion: the criterion object for the training process.
    """

    loop = tqdm(data_iterator, total=len(data_iterator), leave=False)
    dev = get_available_device()
    # losses = []
    # accuracies = []
    for sentences_batch, labels_batch in loop:
        model.train()
        labels_batch = labels_batch.view((labels_batch.shape[0], 1)).to(dev)
        sentences_batch = sentences_batch.to(dev)

        pred = model(sentences_batch)
        loss = criterion(pred, labels_batch).to(get_available_device())

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        model.eval()
        accuracy = binary_accuracy(model.predict(sentences_batch), labels_batch)
        loop.set_description(f"Epoch {epoch_num}")
        loop.set_postfix(loss=loss.item(), acc=accuracy.item())
        # losses.append((loss.item(), labels_batch.shape[0]))
        # accuracies.append((accuracy.item(), labels_batch.shape[0]))

    # return get_mean_metric(losses), get_mean_metric(accuracies)
    return evaluate(model, data_iterator, criterion)


def get_mean_metric(results):
    vals, nums = zip(*results)
    res = np.sum(np.multiply(vals, nums)) / np.sum(nums)
    return res


def evaluate(model, data_iterator, criterion):
    """
    evaluate the model performance on the given data
    :param model: one of our models..
    :param data_iterator: torch data iterator for the relevant subset
    :param criterion: the loss criterion used for evaluation
    :return: tuple of (average loss over all examples, average accuracy over all examples)
    """
    model.eval()
    loss = []
    accuracy = []
    dev = get_available_device()
    with torch.no_grad():
        for sent, labels in data_iterator:
            labels = labels.view((labels.shape[0], 1)).to(dev)
            sent = sent.to(dev)
            loss.append((criterion(model(sent).to(get_available_device()), labels), labels.shape[0]))
            accuracy.append((binary_accuracy(model.predict(sent), labels), labels.shape[0]))

        return get_mean_metric(loss), get_mean_metric(accuracy)


def get_predictions_for_data(model, data_iter):
    """
    This function should iterate over all batches of examples from data_iter and return all of the models
    predictions as a numpy ndarray or torch tensor (or list if you prefer). the prediction should be in the
    same order of the examples returned by data_iter.
    :param model: one of the models you implemented in the exercise
    :param data_iter: torch iterator as given by the DataManager
    :return:
    """
    res = []
    res_logits = []
    model.eval()
    dev = get_available_device()
    sig = nn.Sigmoid()
    with torch.no_grad():
        for sent_batch, _ in data_iter:
            logits = model(sent_batch.to(dev))
            res_logits.extend(logits)
            res.extend(torch.round(sig(logits)))
        return torch.Tensor(res), torch.Tensor(res_logits)


def train_model(model, data_manager, n_epochs, lr, weight_decay=0.):
    """
    Runs the full training procedure for the given model. The optimization should be done using the Adam
    optimizer with all parameters but learning rate and weight decay set to default.
    :param model: module of one of the models implemented in the exercise
    :param data_manager: the DataManager object
    :param n_epochs: number of times to go over the whole training set
    :param lr: learning rate to be used for optimization
    :param weight_decay: parameter for l2 regularization
    """
    loss_func = F.binary_cross_entropy_with_logits
    opt = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    train_metrics, val_metrics = {LOSS: [], ACCURACY: []}, {LOSS: [], ACCURACY: []}

    for epoch in range(1, n_epochs + 1):
        print(f"Epoch {epoch}/{n_epochs}: ", end=',')
        t_loss, t_acc = train_epoch(model, data_manager.get_torch_iterator(TRAIN), 
                                    opt, loss_func, epoch)
        print(f"loss: {t_loss}, accuracy: {t_acc}", end=', ')

        v_loss, v_acc = evaluate(model, data_manager.get_torch_iterator(VAL), loss_func)
        print(f"val_loss: {v_loss}, val_accuracy: {v_acc}")

        train_metrics[LOSS].append(t_loss)
        train_metrics[ACCURACY].append(t_acc)
        val_metrics[LOSS].append(v_loss)
        val_metrics[ACCURACY].append(v_acc)

    return train_metrics, val_metrics


def plot_metrics(train_metrics, val_metrics, label):
    metrics = [LOSS, ACCURACY]
    for n, metric in enumerate(metrics):
        name = metric.replace("_", " ").capitalize()
        plt.subplot(1, 2, n+1)
        n_epochs = np.arange(len(train_metrics[metric]))
        plt.plot(n_epochs, train_metrics[metric], color=colors[0], label='Train')
        plt.plot(n_epochs, val_metrics[metric], color=colors[0], linestyle="--", label='Val')
        plt.xlabel('Epoch')
        plt.ylabel(name)
        if metric == 'loss':
            plt.ylim([0, plt.ylim()[1] + 0.1])
        else:
            plt.ylim([0, 1])

        plt.legend()

    plt.suptitle(label)
    plt.subplots_adjust(top=0.8)
    plt.show()


def get_test_results(model, data_manager):
    print("*** Evaluation: ***")
    test_pred, test_logits = get_predictions_for_data(model, data_manager.get_torch_iterator(TEST))
    y_labels = torch.Tensor(data_manager.get_labels(TEST))
    print("-" * 20)
    print("Test results:")

    print("Loss on Entire Test: ", F.binary_cross_entropy_with_logits(test_logits, y_labels).item())
    print("Accuracy on Entire Test: ", binary_accuracy(test_pred, y_labels).item())
    print("*\n" * 3)
    print("Subset results: ")

    test_set = data_manager.torch_datasets[TEST]
    negated_polarity = data_loader.get_negated_polarity_examples(test_set.data)
    neg_pred, neg_logits = test_pred[negated_polarity], test_logits[negated_polarity]
    neg_labels = y_labels[negated_polarity]
    print("Loss on Negated Polarity: ", F.binary_cross_entropy_with_logits(neg_logits, neg_labels).item())
    print("Accuracy on Negated Polarity: ", binary_accuracy(neg_pred, neg_labels).item())
    
    rare_words = data_loader.get_rare_words_examples(test_set.data, data_manager.sentiment_dataset)
    rare_pred, rare_logits = test_pred[rare_words], test_logits[rare_words]
    rare_labels = y_labels[rare_words]
    print("Loss on Rare Words: ", F.binary_cross_entropy_with_logits(rare_logits, rare_labels).item())
    print("Accuracy on Rare Words: ", binary_accuracy(rare_pred, rare_labels).item())
    print("-" * 20)


def train_log_linear_with_one_hot(n_epochs, lr, weight_decay):
    """
    Here comes your code for training and evaluation of the log linear model with one hot representation.
    """
    name = f"Log-linear average one-hot encoding, n_epochs: {n_epochs}, lr: {lr}, weight_decay: {weight_decay}"
    print("*" * 20)
    print(name)
    print("-" * 20)
    manager = DataManager(data_type=ONEHOT_AVERAGE, batch_size=64)
    vocab_size = len(manager.sentiment_dataset.get_word_counts().keys())
    model = LogLinear(vocab_size).to(get_available_device())
    history = train_model(model, manager, n_epochs, lr=lr, weight_decay=weight_decay)
    plot_metrics(*history, name)
    get_test_results(model, manager)


def train_log_linear_with_w2v(n_epochs, lr, weight_decay):
    """
    Here comes your code for training and evaluation of the log linear model with word embeddings
    representation.
    """
    name = f"Log-linear average Word2Vec encoding, n_epochs: {n_epochs}, lr: {lr}, weight_decay: {weight_decay}"
    print("*" * 20)
    print(name)
    print("-" * 20)
    manager = DataManager(data_type=W2V_AVERAGE, batch_size=64, embedding_dim=W2V_EMBEDDING_DIM)
    model = LogLinear(W2V_EMBEDDING_DIM).to(get_available_device())
    history = train_model(model, manager, n_epochs, lr=lr, weight_decay=weight_decay)
    plot_metrics(*history, name)
    get_test_results(model, manager)


def train_lstm_with_w2v(n_epochs, lr, weight_decay):
    """
    Here comes your code for training and evaluation of the LSTM model.
    """
    name = f"LSTM  Word2Vec encoding, n_epochs: {n_epochs}, lr: {lr}, weight_decay: {weight_decay}"
    print("*" * 20)
    print(name)
    print("-" * 20)
    hidden_dim = 100
    n_layers = 1
    dropout = 0.5
    manager = DataManager(data_type=W2V_SEQUENCE, batch_size=64, embedding_dim=W2V_EMBEDDING_DIM)
    model = LSTM(W2V_EMBEDDING_DIM, hidden_dim, n_layers, dropout).to(get_available_device())
    history = train_model(model, manager, n_epochs, lr=lr, weight_decay= weight_decay)
    plot_metrics(*history, name)
    get_test_results(model, manager)

"""#Log-Linaer One hot avarage model"""

print(get_available_device())
num_epochs = 20
decay = 0.0001
learning_rate = 0.01
train_log_linear_with_one_hot(num_epochs,learning_rate , decay)

"""#Log-Linaer Word2Vec model"""

print(get_available_device())
num_epochs = 20
decay =  0.0001
learning_rate = 0.01
train_log_linear_with_w2v(num_epochs,learning_rate , decay)

"""#LSTM model"""

print(get_available_device())
num_epochs = 20
decay =  0.0001
learning_rate = 0.001
# train_log_linear_with_one_hot(num_epochs, learning_rate, decay)
train_lstm_with_w2v(num_epochs, learning_rate, decay)

